{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задание 2.1 - Нейронные сети\n",
    "\n",
    "В этом задании вы реализуете и натренируете настоящую нейроную сеть своими руками!\n",
    "\n",
    "В некотором смысле это будет расширением прошлого задания - нам нужно просто составить несколько линейных классификаторов вместе!\n",
    "\n",
    "<img src=\"https://i.redd.it/n9fgba8b0qr01.png\" alt=\"Stack_more_layers\" width=\"400px\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import load_svhn, random_split_train_val\n",
    "from gradient_check import check_layer_gradient, check_layer_param_gradient, check_model_gradient\n",
    "from layers import FullyConnectedLayer, ReLULayer\n",
    "from model import TwoLayerNet\n",
    "from trainer import Trainer, Dataset\n",
    "from optim import SGD, MomentumSGD\n",
    "from metrics import multiclass_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Загружаем данные\n",
    "\n",
    "И разделяем их на training и validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "<ipython-input-3-3dff2068dd0c>:2: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  train_flat = train_X.reshape(train_X.shape[0], -1).astype(np.float) / 255.0\n",
      "<ipython-input-3-3dff2068dd0c>:3: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  test_flat = test_X.reshape(test_X.shape[0], -1).astype(np.float) / 255.0\n"
     ]
    }
   ],
   "source": [
    "def prepare_for_neural_network(train_X, test_X):\n",
    "    train_flat = train_X.reshape(train_X.shape[0], -1).astype(np.float) / 255.0\n",
    "    test_flat = test_X.reshape(test_X.shape[0], -1).astype(np.float) / 255.0\n",
    "    \n",
    "    # Subtract mean\n",
    "    mean_image = np.mean(train_flat, axis = 0)\n",
    "    train_flat -= mean_image\n",
    "    test_flat -= mean_image\n",
    "    \n",
    "    return train_flat, test_flat\n",
    "    \n",
    "train_X, train_y, test_X, test_y = load_svhn(\"../assignment1/data\", max_train=10000, max_test=1000)    \n",
    "train_X, test_X = prepare_for_neural_network(train_X, test_X)\n",
    "# Split train into train and val\n",
    "train_X, train_y, val_X, val_y = random_split_train_val(train_X, train_y, num_val = 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Как всегда, начинаем с кирпичиков\n",
    "\n",
    "Мы будем реализовывать необходимые нам слои по очереди. Каждый слой должен реализовать:\n",
    "- прямой проход (forward pass), который генерирует выход слоя по входу и запоминает необходимые данные\n",
    "- обратный проход (backward pass), который получает градиент по выходу слоя и вычисляет градиент по входу и по параметрам\n",
    "\n",
    "Начнем с ReLU, у которого параметров нет."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Gradient check passed!\n"
     ]
    }
   ],
   "source": [
    "# TODO: Implement ReLULayer layer in layers.py\n",
    "# Note: you'll need to copy implementation of the gradient_check function from the previous assignment\n",
    "\n",
    "X = np.array([[1,-2,3],\n",
    "              [-1, 2, 0.1]\n",
    "              ])\n",
    "\n",
    "assert check_layer_gradient(ReLULayer(), X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "А теперь реализуем полносвязный слой (fully connected layer), у которого будет два массива параметров: W (weights) и B (bias).\n",
    "\n",
    "Все параметры наши слои будут использовать для параметров специальный класс `Param`, в котором будут храниться значения параметров и градиенты этих параметров, вычисляемые во время обратного прохода.\n",
    "\n",
    "Это даст возможность аккумулировать (суммировать) градиенты из разных частей функции потерь, например, из cross-entropy loss и regularization loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Gradient check passed!\nGradient check passed!\nGradient check passed!\n"
     ]
    }
   ],
   "source": [
    "# TODO: Implement FullyConnected layer forward and backward methods\n",
    "assert check_layer_gradient(FullyConnectedLayer(3, 4), X)\n",
    "# TODO: Implement storing gradients for W and B\n",
    "assert check_layer_param_gradient(FullyConnectedLayer(3, 4), X, 'W')\n",
    "assert check_layer_param_gradient(FullyConnectedLayer(3, 4), X, 'B')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Создаем нейронную сеть\n",
    "\n",
    "Теперь мы реализуем простейшую нейронную сеть с двумя полносвязным слоями и нелинейностью ReLU. Реализуйте функцию `compute_loss_and_gradients`, она должна запустить прямой и обратный проход через оба слоя для вычисления градиентов.\n",
    "\n",
    "Не забудьте реализовать очистку градиентов в начале функции."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Checking gradient for linear_1_W\n",
      "Gradient check passed!\n",
      "Checking gradient for linear_1_B\n",
      "Gradient check passed!\n",
      "Checking gradient for linear_2_W\n",
      "Gradient check passed!\n",
      "Checking gradient for linear_2_B\n",
      "Gradient check passed!\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "execution_count": 27
    }
   ],
   "source": [
    "# TODO: In model.py, implement compute_loss_and_gradients function\n",
    "model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 3, reg = 0)\n",
    "loss = model.compute_loss_and_gradients(train_X[:2], train_y[:2])\n",
    "\n",
    "# TODO Now implement backward pass and aggregate all of the params\n",
    "check_model_gradient(model, train_X[:2], train_y[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь добавьте к модели регуляризацию - она должна прибавляться к loss и делать свой вклад в градиенты."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Checking gradient for linear_1_W\n",
      "Gradient check passed!\n",
      "Checking gradient for linear_1_B\n",
      "Gradient check passed!\n",
      "Checking gradient for linear_2_W\n",
      "Gradient check passed!\n",
      "Checking gradient for linear_2_B\n",
      "Gradient check passed!\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "execution_count": 22
    }
   ],
   "source": [
    "# TODO Now implement l2 regularization in the forward and backward pass\n",
    "model_with_reg = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 3, reg = 1e1)\n",
    "loss_with_reg = model_with_reg.compute_loss_and_gradients(train_X[5:10], train_y[5:10])\n",
    "assert loss_with_reg > loss and not np.isclose(loss_with_reg, loss), \\\n",
    "    \"Loss with regularization (%2.4f) should be higher than without it (%2.4f)!\" % (loss, loss_with_reg)\n",
    "\n",
    "check_model_gradient(model_with_reg, train_X[5:10], train_y[5:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Также реализуем функцию предсказания (вычисления значения) модели на новых данных.\n",
    "\n",
    "Какое значение точности мы ожидаем увидеть до начала тренировки?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.1"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "# Finally, implement predict function!\n",
    "\n",
    "# TODO: Implement predict function\n",
    "# What would be the value we expect?\n",
    "multiclass_accuracy(model_with_reg.predict(train_X[:30]), train_y[:30]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Допишем код для процесса тренировки\n",
    "\n",
    "Если все реализовано корректно, значение функции ошибки должно уменьшаться с каждой эпохой, пусть и медленно. Не беспокойтесь пока про validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-113-f4364dceba81>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# TODO Implement missing pieces in Trainer.fit function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m# You should expect loss to go down every epoch, even if it's slow\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mloss_history\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_history\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_history\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\petya\\Documents\\projects\\dlcourse_ai\\assignments\\assignment2\\trainer.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    103\u001b[0m                 \u001b[0mbatch_y\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_y\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbatch_indices\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    104\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 105\u001b[1;33m                 \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompute_loss_and_gradients\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_X\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    106\u001b[0m                 \u001b[1;31m#\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\petya\\Documents\\projects\\dlcourse_ai\\assignments\\assignment2\\model.py\u001b[0m in \u001b[0;36mcompute_loss_and_gradients\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m     41\u001b[0m         \u001b[0mparams\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mpar\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 43\u001b[1;33m           \u001b[0mpar\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrad\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpar\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     44\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m         \u001b[1;31m# TODO Compute loss and fill param gradients\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mzeros_like\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\petya\\Documents\\projects\\dlcourse_ai\\.venv\\lib\\site-packages\\numpy\\core\\numeric.py\u001b[0m in \u001b[0;36mzeros_like\u001b[1;34m(a, dtype, order, subok, shape)\u001b[0m\n\u001b[0;32m    138\u001b[0m     \u001b[1;31m# needed instead of a 0 to get same result as zeros for for string dtypes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    139\u001b[0m     \u001b[0mz\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mres\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 140\u001b[1;33m     \u001b[0mmultiarray\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopyto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mres\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mz\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcasting\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'unsafe'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    141\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    142\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mcopyto\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 100, reg = 1e1)\n",
    "dataset = Dataset(train_X, train_y, val_X, val_y)\n",
    "trainer = Trainer(model, dataset, SGD(), learning_rate = 1e-2)\n",
    "\n",
    "# TODO Implement missing pieces in Trainer.fit function\n",
    "# You should expect loss to go down every epoch, even if it's slow\n",
    "loss_history, train_history, val_history = trainer.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Loss: 2.304045, Train accuracy: 0.138778, val accuracy: 0.133000\n",
      "Loss: 2.305318, Train accuracy: 0.170222, val accuracy: 0.174000\n",
      "Loss: 2.304309, Train accuracy: 0.131444, val accuracy: 0.131000\n",
      "Loss: 2.333627, Train accuracy: 0.099444, val accuracy: 0.101000\n",
      "Loss: 2.316517, Train accuracy: 0.118556, val accuracy: 0.115000\n",
      "Loss: 2.275625, Train accuracy: 0.157000, val accuracy: 0.172000\n",
      "Loss: 2.323468, Train accuracy: 0.154556, val accuracy: 0.161000\n",
      "Loss: 2.300946, Train accuracy: 0.159444, val accuracy: 0.154000\n",
      "Loss: 2.335730, Train accuracy: 0.127333, val accuracy: 0.112000\n",
      "Loss: 2.318063, Train accuracy: 0.123556, val accuracy: 0.127000\n",
      "Loss: 2.324191, Train accuracy: 0.138111, val accuracy: 0.147000\n",
      "Loss: 2.301464, Train accuracy: 0.093667, val accuracy: 0.098000\n",
      "Loss: 2.292479, Train accuracy: 0.125667, val accuracy: 0.140000\n",
      "Loss: 2.368481, Train accuracy: 0.096111, val accuracy: 0.095000\n",
      "Loss: 2.338286, Train accuracy: 0.134889, val accuracy: 0.161000\n",
      "Loss: 2.289386, Train accuracy: 0.130000, val accuracy: 0.147000\n",
      "Loss: 2.292817, Train accuracy: 0.117111, val accuracy: 0.127000\n",
      "Loss: 2.327739, Train accuracy: 0.132778, val accuracy: 0.134000\n",
      "Loss: 2.335225, Train accuracy: 0.113111, val accuracy: 0.127000\n",
      "Loss: 2.303867, Train accuracy: 0.147556, val accuracy: 0.154000\n"
     ]
    }
   ],
   "source": [
    "from model import OneLayerNet\n",
    "\n",
    "model = OneLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 100, reg = 1e1)\n",
    "dataset = Dataset(train_X, train_y, val_X, val_y)\n",
    "trainer = Trainer(model, dataset, SGD(), learning_rate = 1e-2)\n",
    "\n",
    "# TODO Implement missing pieces in Trainer.fit function\n",
    "# You should expect loss to go down every epoch, even if it's slow\n",
    "loss_history, train_history, val_history = trainer.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Gradient check passed!\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([1., 2., 0.])"
      ]
     },
     "metadata": {},
     "execution_count": 97
    }
   ],
   "source": [
    "from layers import ReLULayer\n",
    "r = ReLULayer()\n",
    "x = np.array([[1,2,3],[0.1,-2,-3],[-3,2,-5]], dtype=float)\n",
    "check_layer_gradient(r, x)\n",
    "r.dX\n",
    "r.forward(np.array([1,2,-2], dtype=float))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Loss: 1.386644, Train accuracy: 0.000000, val accuracy: 0.000000\n",
      "Loss: 1.386644, Train accuracy: 0.250000, val accuracy: 0.250000\n",
      "Loss: 1.386644, Train accuracy: 0.250000, val accuracy: 0.250000\n",
      "Loss: 1.385479, Train accuracy: 0.250000, val accuracy: 0.250000\n",
      "Loss: 1.386644, Train accuracy: 0.250000, val accuracy: 0.250000\n",
      "Loss: 1.384027, Train accuracy: 0.250000, val accuracy: 0.250000\n",
      "Loss: 1.386644, Train accuracy: 0.250000, val accuracy: 0.250000\n",
      "Loss: 1.385280, Train accuracy: 0.250000, val accuracy: 0.250000\n",
      "Loss: 1.386643, Train accuracy: 0.250000, val accuracy: 0.250000\n",
      "Loss: 1.385180, Train accuracy: 0.250000, val accuracy: 0.250000\n",
      "Loss: 1.383529, Train accuracy: 0.250000, val accuracy: 0.250000\n",
      "Loss: 1.383430, Train accuracy: 0.250000, val accuracy: 0.250000\n",
      "Loss: 1.386643, Train accuracy: 0.250000, val accuracy: 0.250000\n",
      "Loss: 1.383231, Train accuracy: 0.250000, val accuracy: 0.250000\n",
      "Loss: 1.384931, Train accuracy: 0.250000, val accuracy: 0.250000\n",
      "Loss: 1.386643, Train accuracy: 0.250000, val accuracy: 0.250000\n",
      "Loss: 1.387895, Train accuracy: 0.250000, val accuracy: 0.250000\n",
      "Loss: 1.384782, Train accuracy: 0.250000, val accuracy: 0.250000\n",
      "Loss: 1.384733, Train accuracy: 0.250000, val accuracy: 0.250000\n",
      "Loss: 1.384683, Train accuracy: 0.250000, val accuracy: 0.250000\n",
      "Loss: 1.387694, Train accuracy: 0.250000, val accuracy: 0.250000\n",
      "Loss: 1.386642, Train accuracy: 0.250000, val accuracy: 0.250000\n",
      "Loss: 1.387593, Train accuracy: 0.250000, val accuracy: 0.250000\n",
      "Loss: 1.387543, Train accuracy: 0.250000, val accuracy: 0.250000\n",
      "Loss: 1.386642, Train accuracy: 0.250000, val accuracy: 0.250000\n",
      "Loss: 1.384385, Train accuracy: 0.250000, val accuracy: 0.250000\n",
      "Loss: 1.387392, Train accuracy: 0.250000, val accuracy: 0.250000\n",
      "Loss: 1.384285, Train accuracy: 0.250000, val accuracy: 0.250000\n",
      "Loss: 1.386641, Train accuracy: 0.250000, val accuracy: 0.250000\n",
      "Loss: 1.381644, Train accuracy: 0.250000, val accuracy: 0.250000\n",
      "Loss: 1.384136, Train accuracy: 0.250000, val accuracy: 0.250000\n",
      "Loss: 1.387141, Train accuracy: 0.250000, val accuracy: 0.250000\n",
      "Loss: 1.381347, Train accuracy: 0.250000, val accuracy: 0.250000\n",
      "Loss: 1.381248, Train accuracy: 0.250000, val accuracy: 0.250000\n",
      "Loss: 1.383938, Train accuracy: 0.250000, val accuracy: 0.250000\n",
      "Loss: 1.381050, Train accuracy: 0.250000, val accuracy: 0.250000\n",
      "Loss: 1.386639, Train accuracy: 0.250000, val accuracy: 0.250000\n",
      "Loss: 1.386639, Train accuracy: 0.250000, val accuracy: 0.250000\n",
      "Loss: 1.380753, Train accuracy: 0.250000, val accuracy: 0.250000\n",
      "Loss: 1.386639, Train accuracy: 0.250000, val accuracy: 0.250000\n",
      "Loss: 1.386639, Train accuracy: 0.250000, val accuracy: 0.250000\n",
      "Loss: 1.386638, Train accuracy: 0.250000, val accuracy: 0.250000\n",
      "Loss: 1.386638, Train accuracy: 0.250000, val accuracy: 0.250000\n",
      "Loss: 1.386537, Train accuracy: 0.250000, val accuracy: 0.250000\n",
      "Loss: 1.383441, Train accuracy: 0.250000, val accuracy: 0.250000\n",
      "Loss: 1.383392, Train accuracy: 0.250000, val accuracy: 0.250000\n",
      "Loss: 1.383342, Train accuracy: 0.250000, val accuracy: 0.250000\n",
      "Loss: 1.386336, Train accuracy: 0.250000, val accuracy: 0.250000\n",
      "Loss: 1.386637, Train accuracy: 0.250000, val accuracy: 0.250000\n",
      "Loss: 1.379666, Train accuracy: 0.250000, val accuracy: 0.250000\n",
      "Loss: 1.383144, Train accuracy: 0.250000, val accuracy: 0.250000\n",
      "Loss: 1.383094, Train accuracy: 0.250000, val accuracy: 0.250000\n",
      "Loss: 1.386635, Train accuracy: 0.250000, val accuracy: 0.250000\n",
      "Loss: 1.386635, Train accuracy: 0.250000, val accuracy: 0.250000\n",
      "Loss: 1.386635, Train accuracy: 0.250000, val accuracy: 0.250000\n",
      "Loss: 1.379074, Train accuracy: 0.250000, val accuracy: 0.250000\n",
      "Loss: 1.382846, Train accuracy: 0.250000, val accuracy: 0.250000\n",
      "Loss: 1.378877, Train accuracy: 0.250000, val accuracy: 0.250000\n",
      "Loss: 1.386634, Train accuracy: 0.250000, val accuracy: 0.250000\n",
      "Loss: 1.378680, Train accuracy: 0.250000, val accuracy: 0.250000\n",
      "Loss: 1.382648, Train accuracy: 0.250000, val accuracy: 0.250000\n",
      "Loss: 1.378482, Train accuracy: 0.250000, val accuracy: 0.250000\n",
      "Loss: 1.382549, Train accuracy: 0.250000, val accuracy: 0.250000\n",
      "Loss: 1.382499, Train accuracy: 0.250000, val accuracy: 0.250000\n",
      "Loss: 1.386632, Train accuracy: 0.250000, val accuracy: 0.250000\n",
      "Loss: 1.382400, Train accuracy: 0.250000, val accuracy: 0.250000\n",
      "Loss: 1.385383, Train accuracy: 0.250000, val accuracy: 0.250000\n",
      "Loss: 1.382301, Train accuracy: 0.250000, val accuracy: 0.250000\n",
      "Loss: 1.382252, Train accuracy: 0.250000, val accuracy: 0.250000\n",
      "Loss: 1.377695, Train accuracy: 0.250000, val accuracy: 0.250000\n",
      "Loss: 1.385182, Train accuracy: 0.250000, val accuracy: 0.250000\n",
      "Loss: 1.377498, Train accuracy: 0.250000, val accuracy: 0.250000\n",
      "Loss: 1.385082, Train accuracy: 0.250000, val accuracy: 0.250000\n",
      "Loss: 1.386628, Train accuracy: 0.250000, val accuracy: 0.250000\n",
      "Loss: 1.386628, Train accuracy: 0.250000, val accuracy: 0.250000\n",
      "Loss: 1.381905, Train accuracy: 0.250000, val accuracy: 0.250000\n",
      "Loss: 1.386627, Train accuracy: 0.250000, val accuracy: 0.250000\n",
      "Loss: 1.376908, Train accuracy: 0.250000, val accuracy: 0.250000\n",
      "Loss: 1.386626, Train accuracy: 0.250000, val accuracy: 0.250000\n",
      "Loss: 1.381707, Train accuracy: 0.250000, val accuracy: 0.250000\n",
      "Loss: 1.384681, Train accuracy: 0.250000, val accuracy: 0.250000\n",
      "Loss: 1.386625, Train accuracy: 0.250000, val accuracy: 0.250000\n",
      "Loss: 1.384581, Train accuracy: 0.250000, val accuracy: 0.250000\n",
      "Loss: 1.384531, Train accuracy: 0.250000, val accuracy: 0.250000\n",
      "Loss: 1.384481, Train accuracy: 0.250000, val accuracy: 0.250000\n",
      "Loss: 1.381410, Train accuracy: 0.250000, val accuracy: 0.250000\n",
      "Loss: 1.381361, Train accuracy: 0.250000, val accuracy: 0.250000\n",
      "Loss: 1.384330, Train accuracy: 0.250000, val accuracy: 0.250000\n",
      "Loss: 1.375829, Train accuracy: 0.250000, val accuracy: 0.250000\n",
      "Loss: 1.384230, Train accuracy: 0.250000, val accuracy: 0.250000\n",
      "Loss: 1.375633, Train accuracy: 0.250000, val accuracy: 0.250000\n",
      "Loss: 1.381113, Train accuracy: 0.250000, val accuracy: 0.250000\n",
      "Loss: 1.381064, Train accuracy: 0.250000, val accuracy: 0.250000\n",
      "Loss: 1.375339, Train accuracy: 0.250000, val accuracy: 0.250000\n",
      "Loss: 1.383980, Train accuracy: 0.250000, val accuracy: 0.250000\n",
      "Loss: 1.375143, Train accuracy: 0.250000, val accuracy: 0.250000\n",
      "Loss: 1.383880, Train accuracy: 0.250000, val accuracy: 0.250000\n",
      "Loss: 1.386617, Train accuracy: 0.250000, val accuracy: 0.250000\n",
      "Loss: 1.374849, Train accuracy: 0.250000, val accuracy: 0.250000\n",
      "Loss: 1.374751, Train accuracy: 0.250000, val accuracy: 0.250000\n",
      "Loss: 1.380669, Train accuracy: 0.250000, val accuracy: 0.250000\n",
      "Loss: 1.386615, Train accuracy: 0.250000, val accuracy: 0.250000\n",
      "Loss: 1.383580, Train accuracy: 0.250000, val accuracy: 0.250000\n",
      "Loss: 1.380520, Train accuracy: 0.250000, val accuracy: 0.250000\n",
      "Loss: 1.383479, Train accuracy: 0.250000, val accuracy: 0.250000\n",
      "Loss: 1.386613, Train accuracy: 0.250000, val accuracy: 0.250000\n",
      "Loss: 1.380372, Train accuracy: 0.250000, val accuracy: 0.250000\n",
      "Loss: 1.383329, Train accuracy: 0.250000, val accuracy: 0.250000\n",
      "Loss: 1.380274, Train accuracy: 0.250000, val accuracy: 0.250000\n",
      "Loss: 1.373773, Train accuracy: 0.250000, val accuracy: 0.250000\n",
      "Loss: 1.380175, Train accuracy: 0.250000, val accuracy: 0.250000\n",
      "Loss: 1.373578, Train accuracy: 0.250000, val accuracy: 0.250000\n",
      "Loss: 1.373480, Train accuracy: 0.250000, val accuracy: 0.250000\n",
      "Loss: 1.383029, Train accuracy: 0.250000, val accuracy: 0.250000\n",
      "Loss: 1.379977, Train accuracy: 0.250000, val accuracy: 0.250000\n",
      "Loss: 1.373187, Train accuracy: 0.250000, val accuracy: 0.250000\n",
      "Loss: 1.386607, Train accuracy: 0.250000, val accuracy: 0.250000\n",
      "Loss: 1.386606, Train accuracy: 0.250000, val accuracy: 0.250000\n",
      "Loss: 1.379780, Train accuracy: 0.250000, val accuracy: 0.250000\n",
      "Loss: 1.386605, Train accuracy: 0.250000, val accuracy: 0.250000\n",
      "Loss: 1.386604, Train accuracy: 0.250000, val accuracy: 0.250000\n",
      "Loss: 1.386603, Train accuracy: 0.250000, val accuracy: 0.250000\n",
      "Loss: 1.386603, Train accuracy: 0.250000, val accuracy: 0.250000\n",
      "Loss: 1.386602, Train accuracy: 0.250000, val accuracy: 0.250000\n",
      "Loss: 1.382480, Train accuracy: 0.250000, val accuracy: 0.250000\n",
      "Loss: 1.372212, Train accuracy: 0.250000, val accuracy: 0.250000\n",
      "Loss: 1.379385, Train accuracy: 0.250000, val accuracy: 0.250000\n",
      "Loss: 1.386600, Train accuracy: 0.250000, val accuracy: 0.250000\n",
      "Loss: 1.386599, Train accuracy: 0.250000, val accuracy: 0.250000\n",
      "Loss: 1.379237, Train accuracy: 0.250000, val accuracy: 0.250000\n",
      "Loss: 1.371725, Train accuracy: 0.250000, val accuracy: 0.250000\n",
      "Loss: 1.379139, Train accuracy: 0.250000, val accuracy: 0.250000\n",
      "Loss: 1.371530, Train accuracy: 0.250000, val accuracy: 0.250000\n",
      "Loss: 1.382031, Train accuracy: 0.250000, val accuracy: 0.250000\n",
      "Loss: 1.371336, Train accuracy: 0.250000, val accuracy: 0.250000\n",
      "Loss: 1.378941, Train accuracy: 0.250000, val accuracy: 0.250000\n",
      "Loss: 1.371141, Train accuracy: 0.250000, val accuracy: 0.250000\n",
      "Loss: 1.371044, Train accuracy: 0.250000, val accuracy: 0.250000\n",
      "Loss: 1.378794, Train accuracy: 0.250000, val accuracy: 0.250000\n",
      "Loss: 1.386591, Train accuracy: 0.250000, val accuracy: 0.250000\n",
      "Loss: 1.378695, Train accuracy: 0.250000, val accuracy: 0.250000\n",
      "Loss: 1.370655, Train accuracy: 0.250000, val accuracy: 0.250000\n",
      "Loss: 1.370558, Train accuracy: 0.250000, val accuracy: 0.250000\n",
      "Loss: 1.381532, Train accuracy: 0.250000, val accuracy: 0.250000\n",
      "Loss: 1.378498, Train accuracy: 0.250000, val accuracy: 0.250000\n",
      "Loss: 1.378449, Train accuracy: 0.250000, val accuracy: 0.250000\n",
      "Loss: 1.378400, Train accuracy: 0.250000, val accuracy: 0.250000\n",
      "Loss: 1.378350, Train accuracy: 0.250000, val accuracy: 0.250000\n",
      "Loss: 1.386585, Train accuracy: 0.250000, val accuracy: 0.250000\n",
      "Loss: 1.386584, Train accuracy: 0.250000, val accuracy: 0.250000\n",
      "Loss: 1.369782, Train accuracy: 0.250000, val accuracy: 0.250000\n",
      "Loss: 1.381133, Train accuracy: 0.250000, val accuracy: 0.250000\n",
      "Loss: 1.369588, Train accuracy: 0.250000, val accuracy: 0.250000\n",
      "Loss: 1.369491, Train accuracy: 0.250000, val accuracy: 0.250000\n",
      "Loss: 1.380983, Train accuracy: 0.250000, val accuracy: 0.250000\n",
      "Loss: 1.369297, Train accuracy: 0.250000, val accuracy: 0.250000\n",
      "Loss: 1.369200, Train accuracy: 0.250000, val accuracy: 0.250000\n",
      "Loss: 1.380834, Train accuracy: 0.250000, val accuracy: 0.250000\n",
      "Loss: 1.369006, Train accuracy: 0.250000, val accuracy: 0.250000\n",
      "Loss: 1.368909, Train accuracy: 0.250000, val accuracy: 0.250000\n",
      "Loss: 1.380684, Train accuracy: 0.250000, val accuracy: 0.250000\n",
      "Loss: 1.386574, Train accuracy: 0.250000, val accuracy: 0.250000\n",
      "Loss: 1.377612, Train accuracy: 0.250000, val accuracy: 0.250000\n",
      "Loss: 1.377563, Train accuracy: 0.250000, val accuracy: 0.250000\n",
      "Loss: 1.377514, Train accuracy: 0.250000, val accuracy: 0.250000\n",
      "Loss: 1.368328, Train accuracy: 0.250000, val accuracy: 0.250000\n",
      "Loss: 1.368232, Train accuracy: 0.250000, val accuracy: 0.250000\n",
      "Loss: 1.377366, Train accuracy: 0.250000, val accuracy: 0.250000\n",
      "Loss: 1.386568, Train accuracy: 0.250000, val accuracy: 0.250000\n",
      "Loss: 1.367942, Train accuracy: 0.250000, val accuracy: 0.250000\n",
      "Loss: 1.386567, Train accuracy: 0.250000, val accuracy: 0.250000\n",
      "Loss: 1.377169, Train accuracy: 0.250000, val accuracy: 0.250000\n",
      "Loss: 1.386565, Train accuracy: 0.250000, val accuracy: 0.250000\n",
      "Loss: 1.377071, Train accuracy: 0.250000, val accuracy: 0.250000\n",
      "Loss: 1.379987, Train accuracy: 0.250000, val accuracy: 0.250000\n",
      "Loss: 1.376973, Train accuracy: 0.250000, val accuracy: 0.250000\n",
      "Loss: 1.386561, Train accuracy: 0.250000, val accuracy: 0.250000\n",
      "Loss: 1.379838, Train accuracy: 0.250000, val accuracy: 0.250000\n",
      "Loss: 1.376825, Train accuracy: 0.250000, val accuracy: 0.250000\n",
      "Loss: 1.379739, Train accuracy: 0.250000, val accuracy: 0.250000\n",
      "Loss: 1.366879, Train accuracy: 0.250000, val accuracy: 0.250000\n",
      "Loss: 1.386557, Train accuracy: 0.250000, val accuracy: 0.250000\n",
      "Loss: 1.376629, Train accuracy: 0.250000, val accuracy: 0.250000\n",
      "Loss: 1.379540, Train accuracy: 0.250000, val accuracy: 0.250000\n",
      "Loss: 1.379490, Train accuracy: 0.250000, val accuracy: 0.250000\n",
      "Loss: 1.386553, Train accuracy: 0.250000, val accuracy: 0.250000\n",
      "Loss: 1.386552, Train accuracy: 0.250000, val accuracy: 0.250000\n",
      "Loss: 1.376383, Train accuracy: 0.250000, val accuracy: 0.250000\n",
      "Loss: 1.379291, Train accuracy: 0.250000, val accuracy: 0.250000\n",
      "Loss: 1.376285, Train accuracy: 0.250000, val accuracy: 0.250000\n",
      "Loss: 1.386548, Train accuracy: 0.250000, val accuracy: 0.250000\n",
      "Loss: 1.365819, Train accuracy: 0.250000, val accuracy: 0.250000\n",
      "Loss: 1.376138, Train accuracy: 0.250000, val accuracy: 0.250000\n",
      "Loss: 1.376089, Train accuracy: 0.250000, val accuracy: 0.250000\n",
      "Loss: 1.376040, Train accuracy: 0.250000, val accuracy: 0.250000\n",
      "Loss: 1.386544, Train accuracy: 0.250000, val accuracy: 0.250000\n",
      "Loss: 1.386543, Train accuracy: 0.250000, val accuracy: 0.250000\n",
      "Loss: 1.375893, Train accuracy: 0.250000, val accuracy: 0.250000\n",
      "Loss: 1.386541, Train accuracy: 0.250000, val accuracy: 0.250000\n",
      "Loss: 1.365049, Train accuracy: 0.250000, val accuracy: 0.250000\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'linear_1_W': <layers.Param at 0x1bcbb8779a0>,\n",
       " 'linear_1_B': <layers.Param at 0x1bcbb8776d0>}"
      ]
     },
     "metadata": {},
     "execution_count": 114
    }
   ],
   "source": [
    "model = OneLayerNet(n_input = 2, n_output = 4, hidden_layer_size = 10, reg = 0)\n",
    "dataset = Dataset(np.array([[0,0], [0,1], [1,0], [1,1]]),\n",
    "                    np.array([0,1,2,3]), \n",
    "                    np.array([[0,0], [0,1], [1,0], [1,1]]),\n",
    "                    np.array([0,1,2,3])\n",
    "                    )\n",
    "trainer = Trainer(model, dataset, SGD(), learning_rate = 1e-4, batch_size=1, num_epochs=200)\n",
    "\n",
    "# TODO Implement missing pieces in Trainer.fit function\n",
    "# You should expect loss to go down every epoch, even if it's slow\n",
    "loss_history, train_history, val_history = trainer.fit()\n",
    "model.params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Loss: 1.386831, Train accuracy: 0.250000, val accuracy: 0.250000\n[[-3.71569086e-05  5.02231275e-04 -1.29463346e-03 -8.94378595e-04\n   1.26249876e-03  3.44281436e-05 -1.67781688e-03 -2.90937595e-04\n   1.74909653e-04  4.70344631e-04]]\n[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "model = TwoLayerNet(n_input = 2, n_output = 4, hidden_layer_size = 10, reg = 0)\n",
    "dataset = Dataset(np.array([[0,0], [0,1], [1,0], [1,1]]),\n",
    "                    np.array([0,1,2,3]), \n",
    "                    np.array([[0,0], [0,1], [1,0], [1,1]]),\n",
    "                    np.array([0,1,2,3])\n",
    "                    )\n",
    "trainer = Trainer(model, dataset, SGD(), learning_rate = 1e-4, batch_size=1, num_epochs=1)\n",
    "\n",
    "# TODO Implement missing pieces in Trainer.fit function\n",
    "# You should expect loss to go down every epoch, even if it's slow\n",
    "loss_history, train_history, val_history = trainer.fit()\n",
    "# print(model.params()['linear_1_B'].value)\n",
    "# p = model.params()\n",
    "# p['linear_1_B'].value = np.zeros_like(model.params()['linear_1_B'].value)\n",
    "# print(model.params()['linear_1_B'].value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0], dtype=int64)"
      ]
     },
     "metadata": {},
     "execution_count": 73
    }
   ],
   "source": [
    "model.predict(np.array([[0,0], [0,1], [1,0], [1,1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "linear_1_W: [[ 3.09012500e-08 -1.77842373e-07]\n [ 5.55417550e-08 -7.10245543e-08]]\nlinear_1_B: [[3.87971842e-08 9.50328659e-08]]\nlinear_2_W: [[-2.21426754e-07 -2.00618510e-08 -8.36578081e-09 -3.11341442e-08]\n [-1.09457498e-07  6.61053296e-08 -2.13037927e-08 -1.34646267e-07]]\nlinear_2_B: [[ 0.00196292 -0.00022334 -0.00031654 -0.00142303]]\n"
     ]
    }
   ],
   "source": [
    "for name, par in model.params().items():\n",
    "    print(f\"{name}: {par.value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_history)\n",
    "plt.plot(val_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Улучшаем процесс тренировки\n",
    "\n",
    "Мы реализуем несколько ключевых оптимизаций, необходимых для тренировки современных нейросетей."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Уменьшение скорости обучения (learning rate decay)\n",
    "\n",
    "Одна из необходимых оптимизаций во время тренировки нейронных сетей - постепенное уменьшение скорости обучения по мере тренировки.\n",
    "\n",
    "Один из стандартных методов - уменьшение скорости обучения (learning rate) каждые N эпох на коэффициент d (часто называемый decay). Значения N и d, как всегда, являются гиперпараметрами и должны подбираться на основе эффективности на проверочных данных (validation data). \n",
    "\n",
    "В нашем случае N будет равным 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Loss: 2.303905, Train accuracy: 0.196667, val accuracy: 0.206000\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-2cd0d7e60500>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0minitial_learning_rate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mloss_history\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_history\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_history\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;32massert\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearning_rate\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0minitial_learning_rate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Learning rate should've been reduced\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\petya\\Documents\\projects\\dlcourse_ai\\assignments\\assignment2\\trainer.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    103\u001b[0m                 \u001b[0mbatch_y\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_y\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbatch_indices\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    104\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 105\u001b[1;33m                 \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompute_loss_and_gradients\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_X\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    106\u001b[0m                 \u001b[1;31m#\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\petya\\Documents\\projects\\dlcourse_ai\\assignments\\assignment2\\model.py\u001b[0m in \u001b[0;36mcompute_loss_and_gradients\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m     49\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mn_lay\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlay\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m           \u001b[1;32mif\u001b[0m \u001b[0mn_lay\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 51\u001b[1;33m             \u001b[0mcurrent_X\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlay\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     52\u001b[0m           \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m             \u001b[0mcurrent_X\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlay\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcurrent_X\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\petya\\Documents\\projects\\dlcourse_ai\\assignments\\assignment2\\layers.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    159\u001b[0m         \u001b[1;31m# Your final implementation shouldn't have any loops\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 161\u001b[1;33m         \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m \u001b[1;33m@\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mW\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mB\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    162\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# TODO Implement learning rate decay inside Trainer.fit method\n",
    "# Decay should happen once per epoch\n",
    "\n",
    "model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 100, reg = 1e-1)\n",
    "dataset = Dataset(train_X, train_y, val_X, val_y)\n",
    "trainer = Trainer(model, dataset, SGD(), learning_rate_decay=0.99)\n",
    "\n",
    "initial_learning_rate = trainer.learning_rate\n",
    "loss_history, train_history, val_history = trainer.fit()\n",
    "\n",
    "assert trainer.learning_rate < initial_learning_rate, \"Learning rate should've been reduced\"\n",
    "assert trainer.learning_rate > 0.5*initial_learning_rate, \"Learning rate shouldn'tve been reduced that much!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Накопление импульса (Momentum SGD)\n",
    "\n",
    "Другой большой класс оптимизаций - использование более эффективных методов градиентного спуска. Мы реализуем один из них - накопление импульса (Momentum SGD).\n",
    "\n",
    "Этот метод хранит скорость движения, использует градиент для ее изменения на каждом шаге, и изменяет веса пропорционально значению скорости.\n",
    "(Физическая аналогия: Вместо скорости градиенты теперь будут задавать ускорение, но будет присутствовать сила трения.)\n",
    "\n",
    "```\n",
    "velocity = momentum * velocity - learning_rate * gradient \n",
    "w = w + velocity\n",
    "```\n",
    "\n",
    "`momentum` здесь коэффициент затухания, который тоже является гиперпараметром (к счастью, для него часто есть хорошее значение по умолчанию, типичный диапазон -- 0.8-0.99).\n",
    "\n",
    "Несколько полезных ссылок, где метод разбирается более подробно:  \n",
    "http://cs231n.github.io/neural-networks-3/#sgd  \n",
    "https://distill.pub/2017/momentum/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement MomentumSGD.update function in optim.py\n",
    "\n",
    "model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 100, reg = 1e-1)\n",
    "dataset = Dataset(train_X, train_y, val_X, val_y)\n",
    "trainer = Trainer(model, dataset, MomentumSGD(), learning_rate=1e-4, learning_rate_decay=0.99)\n",
    "\n",
    "# You should see even better results than before!\n",
    "loss_history, train_history, val_history = trainer.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ну что, давайте уже тренировать сеть!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Последний тест - переобучимся (overfit) на маленьком наборе данных\n",
    "\n",
    "Хороший способ проверить, все ли реализовано корректно - переобучить сеть на маленьком наборе данных.  \n",
    "Наша модель обладает достаточной мощностью, чтобы приблизить маленький набор данных идеально, поэтому мы ожидаем, что на нем мы быстро дойдем до 100% точности на тренировочном наборе. \n",
    "\n",
    "Если этого не происходит, то где-то была допущена ошибка!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_size = 15\n",
    "model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 100, reg = 1e-1)\n",
    "dataset = Dataset(train_X[:data_size], train_y[:data_size], val_X[:data_size], val_y[:data_size])\n",
    "trainer = Trainer(model, dataset, SGD(), learning_rate=1e-1, num_epochs=150, batch_size=5)\n",
    "\n",
    "# You should expect this to reach 1.0 training accuracy \n",
    "loss_history, train_history, val_history = trainer.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь найдем гипепараметры, для которых этот процесс сходится быстрее.\n",
    "Если все реализовано корректно, то существуют параметры, при которых процесс сходится в **20** эпох или еще быстрее.\n",
    "Найдите их!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, tweak some hyper parameters and make it train to 1.0 accuracy in 20 epochs or less\n",
    "\n",
    "model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 100, reg = 1e-1)\n",
    "dataset = Dataset(train_X[:data_size], train_y[:data_size], val_X[:data_size], val_y[:data_size])\n",
    "# TODO: Change any hyperparamers or optimizators to reach training accuracy in 20 epochs\n",
    "trainer = Trainer(model, dataset, SGD(), learning_rate=1e-1, num_epochs=20, batch_size=5)\n",
    "\n",
    "loss_history, train_history, val_history = trainer.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Итак, основное мероприятие!\n",
    "\n",
    "Натренируйте лучшую нейросеть! Можно добавлять и изменять параметры, менять количество нейронов в слоях сети и как угодно экспериментировать. \n",
    "\n",
    "Добейтесь точности лучше **60%** на validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Let's train the best one-hidden-layer network we can\n",
    "\n",
    "learning_rates = 1e-4\n",
    "reg_strength = 1e-3\n",
    "learning_rate_decay = 0.999\n",
    "hidden_layer_size = 128\n",
    "num_epochs = 200\n",
    "batch_size = 64\n",
    "\n",
    "best_classifier = None\n",
    "best_val_accuracy = None\n",
    "\n",
    "loss_history = []\n",
    "train_history = []\n",
    "val_history = []\n",
    "\n",
    "# TODO find the best hyperparameters to train the network\n",
    "# Don't hesitate to add new values to the arrays above, perform experiments, use any tricks you want\n",
    "# You should expect to get to at least 40% of valudation accuracy\n",
    "# Save loss/train/history of the best classifier to the variables above\n",
    "\n",
    "print('best validation accuracy achieved: %f' % best_val_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 7))\n",
    "plt.subplot(211)\n",
    "plt.title(\"Loss\")\n",
    "plt.plot(loss_history)\n",
    "plt.subplot(212)\n",
    "plt.title(\"Train/validation accuracy\")\n",
    "plt.plot(train_history)\n",
    "plt.plot(val_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Как обычно, посмотрим, как наша лучшая модель работает на тестовых данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred = best_classifier.predict(test_X)\n",
    "test_accuracy = multiclass_accuracy(test_pred, test_y)\n",
    "print('Neural net test set accuracy: %f' % (test_accuracy, ))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}